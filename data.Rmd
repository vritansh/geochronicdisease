# Data

## Sources

The dataset used in this project is borrowed from the division of population health in <b>Center for Disease Control and Prevention (CDC)</b> - (https://catalog.data.gov/dataset/u-s-chronic-disease-indicators-cdi). 
The dataset was created on 10th November 2020 and was last modified on 21st April 2022.

```{r}
library(dplyr)
library(ggplot2)
library(forcats)
library(tidyr)
library(tibble)
library(choroplethr)
library(tidyverse)
library(mapview)
library(redav)
```

It contains 1082328 entries, where there are 34 attributes contributing to the observance of the chronic disease in the United States of America.

```{r}
    data <- read.csv("/Users/vritansh/Documents/Columbia/EDAV/EDAV_FINAL_PROJECT/USChronicDiseases.csv")
    dim(data)
    ```

Let's have a look at the structure of the dataset in order to comprehend the varied traits that help define the data. 

```{r}
str(data)
```

The attributes that we focus on for this project are:

<li><b>YearStart</b> : Year in which the chronic disease was recorded.</li> 
<li><b>LocationAbbr</b> : State associated with the chronic disease.</li> 
<li><b>DataSource</b> : Contributors for the survey conducted to record the chronic diseases.</li> 
<li><b>Topic</b> : Name of the chronic disease.</li> 
<li><b>Question</b> : Indicators associated with a chronic disease, for example - Mortality, Binge Consumption, etc,.</li> 
<li><b>DataValue</b> : Population that have been affected by a specific indicator.</li> 
<li><b>Stratification1</b> : Describes the gender or race of an individual.</li> 
<li><b>GeoLocation</b> : The latitude and longitude points at which the data was collected.</li>  

</br>
We observe that there are several redundant columns and multiple columns with null values. Since we can't work directly with this dataset, we analyze the missing values and then transform the data into it's usable form.


## Missing value analysis

Now to begin with missing value analysis we first check the percentage of missing values in the initial dataset. Please note here that we are referring to the initial dataset as the original dataset with only the years 2018, 2019, 2020, 2021. Sine there were many null string values in the dataset we converted them to NA before calculating the percentage of missing values in each column.

```{r}
data1 = data 

df5 <- data1[data1$YearStart %in% c('2018', '2019', '2020','2021'), ]

df5[df5 == ''] <- NA

drop <- c("X")
df5 = df5[,!(names(df5) %in% drop)]
(colMeans(is.na(df5)))*100
```
```{r}
data1 = data

df <- data1[data1$YearStart %in% c('2018', '2019', '2020','2021'), ]

df[df == ''] <- NA

drop <- c("X")
df = df[,!(names(df) %in% drop)]

missing.values <- df %>%
  gather(key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  group_by(key) %>%
  mutate(total = n()) %>%
  group_by(key, total, isna) %>%
  summarise(num.isna = n()) %>%
  mutate(pct = num.isna / total * 100)

levels <-
    (missing.values  %>% filter(isna == T) %>% arrange(desc(pct)))$key

percentage.plot <- missing.values %>%
      ggplot() +
        geom_bar(aes(x = reorder(key, desc(pct)), 
                     y = pct, fill=isna), 
                 stat = 'identity', alpha=0.8) +
      scale_x_discrete(limits = levels) +
      scale_fill_manual(name = "", 
                        values = c('steelblue', 'tomato3'), labels = c("Present", "Missing")) +
      coord_flip() +
      labs(title = "Percentage of missing values", x =
             'Variable', y = "% of missing values")

percentage.plot
```


From the above, we can observe that we have missing values in the following columns: </br>
Response, DataValueUnit, DataValue, DataValueAlt, DataValueFootnoteSymbol, DatavalueFootnote, LowConfidenceLimit, HighConfidenceLimit,  StratificationCategory2, Stratification2,  StratificationCategory3, Stratification3, GeoLocation, ResponseID, StratificationCategoryID2,  StratificationID2, StratificationCategoryID3, StratificationID3. 

For the purpose of the further analysis we straightaway dropped columns like - </br>
Response, StratificationCategory2, Stratification2,  StratificationCategory3, Stratification3, ResponseID, StratificationCategoryID2, StratificationID2, StratificationCategoryID3, StratificationID3. We did so as they had 100% of the values missing so keeping them would only add more noise to our plots as they serve no useful meaning. 
</br>
In addition to the above, we also dropped the colummns: DataValueFootnoteSymbol, DatavalueFootnote as these columns have significant (~66%) number of missing values and served no useful purpose for our analysis. 

</br>
Now we recalculate the number of missing values in each column to get an idea of what we have after the above transformations. 

```{r}

#Add missing data
data2 = read.csv(file = '/Users/vritansh/Downloads/CDC.csv') 

df <- data2[data2$YearStart %in% c('2018', '2019', '2020','2021'), ]

drop <- c("X")
df = df[,!(names(df) %in% drop)]

tidydf <- df %>% 
  rownames_to_column("id") %>% 
  gather(key, value, -id) %>% 
  mutate(missing = ifelse(is.na(value), "yes", "no"))

dat2 <- read.csv("/Users/vritansh/Downloads/CDC.csv", header=T, na.strings=c("","NA"))
sapply(dat2, function(x) sum(is.na(x)))
df <- dat2[dat2$YearStart %in% c('2018', '2019', '2020','2021'), ]
missing.values <- df %>%
  gather(key = "key", value = "val") %>%
  mutate(is.missing = is.na(val)) %>%
  group_by(key, is.missing) %>%
  summarise(num.missing = n()) %>%
  filter(is.missing==T) %>%
  select(-is.missing) %>%
  arrange(desc(num.missing)) 
```

To get a better idea about the distribution of the missing values, we plot several types of graphs. This also help us understand patterns between different missing values, if any. 
We begin with plotting a vertical bar chart 
```{r}
ggplot() + geom_bar(data = missing.values, aes(x=key, y=num.missing), stat = 'identity')+labs(x='columns', y="number of missing values", title='Bar Chart of the number of missing values') + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + coord_cartesian(ylim=c(0,100000)) + scale_y_continuous(breaks=seq(0,100000,12500))
```
From the above Bar Chart we can infer that DataValue and DataValueAlt have same number of missing values. Similarly, the columns HighConfidenceLimit and LowConfidenceLimit have equal number of missing values. Perhaps, there is a correlation between them? Before we arrive to this conclusion, let us take look through some more visualizations. 
```{r}

dat2 <- read.csv("/Users/vritansh/Downloads/CDC.csv", header=T, na.strings=c("","NA"))
sapply(dat2, function(x) sum(is.na(x)))
df <- dat2[dat2$YearStart %in% c('2018', '2019', '2020','2021'), ]
missing.values <- df %>%
  gather(key = "key", value = "val") %>%
  mutate(is.missing = is.na(val)) %>%
  group_by(key, is.missing) %>%
  summarise(num.missing = n()) %>%
  filter(is.missing==T) %>%
  select(-is.missing) %>%
  arrange(desc(num.missing)) 

library(redav)
colnames(df) <- c('A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W')
plot_missing(df, percent = FALSE)

```
The above graph gives a more different view to visualize the distribution of missing values. The Variable on the X-axis are the columns. We have one plot where we get an idea of the number of rows missing in each column. Then we have another plot which is giving us a pattern of the missing values in each column. The third plot gives us a more clear picture regarding the association of missing values with the row count. 

We calculated the percentage of missing values in the beginning but we did not do so after dropping the columns so to get a better idea of the percentage of missing values, we plot a horizontal bar chart.

```{r}

missing.values <- df %>%
  gather(key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  group_by(key) %>%
  mutate(total = n()) %>%
  group_by(key, total, isna) %>%
  summarise(num.isna = n()) %>%
  mutate(pct = num.isna / total * 100)

levels <-
    (missing.values  %>% filter(isna == T) %>% arrange(desc(pct)))$key

percentage.plot <- missing.values %>%
      ggplot() +
        geom_bar(aes(x = reorder(key, desc(pct)), 
                     y = pct, fill=isna), 
                 stat = 'identity', alpha=0.8) +
      scale_x_discrete(limits = levels) +
      scale_fill_manual(name = "", 
                        values = c('steelblue', 'tomato3'), labels = c("Present", "Missing")) +
      coord_flip() +
      labs(title = "Percentage of missing values", x =
             'Variable', y = "% of missing values")

percentage.plot
```

As expected, the above horizontal bar chart is consistent with the first bar chart that we made. The only difference here is that we have taken the percentage instead of the number of missing values. In addition to that, the above plot is much more visually appealing and is arranged in ascending order to enable visual comparative analysis easily. 


So far, we had primarily been focussing on the columns. Therefore we now dive into the row level aspects. So now let is visualize the missing values in another way by plotting each row in the dataset to get further insights.  
```{r}
row.plot <- df %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('steelblue', 'tomato3'),
        labels = c("Present", "Missing")) +
    scale_x_discrete(limits = levels) +
    labs(x = "Columns",
           y = "Row Number", title = "Missing values in rows") +
    coord_flip()

row.plot
```
The above visualization affirms that the LowConfidenceLimit and HighConfidenceLimit are indeed correlated. In fact they are most likely the same because the rows that have missing LowConfidenceLimit also have missing HighConfidenceLimit and vice versa. A similar conclusion can be made for DataValue and DataValueAlt. GeoLocation has been consistent in the sense that it has very few missing values.


Since the above plots did not really take other columns (columns without missing values) into consideration, it can be difficult to gauge the relativity between columns that do not have missing values and the columns that have missing values. 
Therefore, the below plot shows all the columns. The bars filled with blue indicates the one's having missing values therefore they are indicated as True. False indicates that the region does not have missing values. Variables represent the columns 
```{r}


df  %>%
  summarise_all(list(~is.na(.)))%>%
  pivot_longer(everything(),
               names_to = "variables", values_to="missing")

df  %>%
  summarise_all(list(~is.na(.)))%>%
  pivot_longer(everything(),
      names_to = "variables", values_to="missing") %>%
  count(variables, missing)

df  %>%
  summarise_all(list(~is.na(.)))%>%
  pivot_longer(everything(),
        names_to = "variables", values_to="missing") %>%
  count(variables, missing) %>%
  ggplot(aes(y=variables,x=n,fill=missing))+
  geom_col()
```

## Cleaning / transformation

Based on the analysis from the plots above, we will be dropping 16 features (mentioned above) that have missing values since it makes more sense than imputing those columns.

```{r}
    data <-dplyr::select(data, -c('StratificationCategoryID2', 'StratificationCategoryID3','StratificationID2','StratificationID3', 'StratificationCategory2','Stratification2','StratificationCategory3','Stratification3', 'LowConfidenceLimit','Response', 'HighConfidenceLimit','ResponseID','DataValueUnit','DataValueAlt','DataValueFootnoteSymbol', 'DatavalueFootnote'))
    str(data)
```

To be more specific on a period of data, we will be trimming the dataset down to the "Pre-During COVID" period i,e 2018 - 2020 to estimate the effect of chronic diseases on the individuals.


    ```{r}
    data <- data[data$YearStart %in% c('2018', '2019', '2020'), ]
    ```

Since there are many categories in the Data Value type, we will extract entries related to "Number" that give us the population count.

    ```{r}
    data <- data[data$DataValueType %in% c("Number"), ]
    ```

    ```{r}
    data$DataValue[data$DataValue==""] <- 0
    data$DataValue <- as.numeric(data$DataValue)
    str(data)
    ```


There are 8 data sources contributing to the collection of chronic data after pre-processing the data,(intially there were 26 contributors). 

    ```{r}
    length(unique(data$DataSource))
    ```

```{r}

    data %>%
      group_by(DataSource) %>%
      summarise(count = n()) %>%
      ggplot(aes(x=fct_reorder(DataSource, count,.desc = FALSE),y=count)) +
      geom_bar(stat = "identity",fill = "orange") +
      ggtitle("DataSource Distribution") +
      coord_flip()+
      xlab("DataSource") + 
      theme(panel.grid.major.x = element_blank()) 

```

From the plot, it is clear that "NVSS" has most contributions, we will be using the data only from that source for our analysis. 

    ```{r}
    data <- data[data$DataSource %in% c("NVSS"), ]
    dim(data)
    str(data)
    ```

The updated data after cleaning and considering the optimal attributes has 10540 entries and 18 attributes that constitutes our primary 8 features along with 10 supporting features for analysis.



